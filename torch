%cd '/content/'                 #경로 설정
os.mkdir('/content/dataset')    #폴더 생성

!wget url #colab에 바로 다운로드하기   ####!wget https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/tywbtsjrjv-1.zip
!unzip -q '파일경로.확장자'            #압출해제

USE_CUDA = torch.cuda.is_available()                    #torch.device() 설정
DEVICE = torch.device('cuda' if USE_CUDA else 'cpu')

## colab에 pakage 설치 konlpy import 하기 
!apt-get update
!apt-get install g++ openjdk-8-jdk 
!pip install konlpy JPype1-py3
!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)

# PC -> colab으로  file upload
from google.colab import files
uploaded = files.upload()
for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))
      

#colab -> PC로  file download
from google.colab import files
files.download('file_name.xxx)

#google drive에서 file download
from google.colab import drive
drive.mount('/gdrive')
!ls /gdrive/My\ Drive/Colab\ Notebooks/data
accidents = pd.read_csv('path + file_name.xxx', delimiter='\t')

#google drive로 file upload
from google.colab import drive
data.to_csv('path + file_name.xxx')


def preprocessing_datacopy(base_path, file_name):
    os.mkdir('/content/dataset')
    original_dataset_dir = os.path.join(base_path, file_name)

    classes_list = os.listdir(original_dataset_dir)

    base_dir = '/content/splitted'
    os.mkdir(base_dir)

    train_dir = os.path.join(base_dir, 'train')
    os.mkdir(train_dir)
    validation_dir = os.path.join(base_dir,'val')
    os.mkdir(validation_dir)
    test_dir = os.path.join(base_dir,'test')
    os.mkdir(test_dir)



    for clss in classes_list:
        os.mkdir(os.path.join(train_dir, clss))
        os.mkdir(os.path.join(validation_dir, clss))
        os.mkdir(os.path.join(test_dir, clss))


    for clss in classes_list:
        path = os.path.join(original_dataset_dir, clss)
        fnames = os.listdir(path)

        train_size = math.floor(len(fnames) * .6)
        validation_size = math.floor(len(fnames) * .2)
        test_size = math.floor(len(fnames) * .2)

        train_fnames = fnames[:train_size]
        print(f'train size: {clss}   file size: {len(train_fnames)}')
        for fname in train_fnames:
            src = os.path.join(path, fname)
            dst = os.path.join(os.path.join(train_dir, clss), fname)
            shutil.copyfile(src, dst)


        validation_fnames = fnames[train_size : validation_size + train_size]
        print(f'validation size: {clss} file size: {len(validation_fnames)}')

        for fname in validation_fnames:
            src = os.path.join(path, fname)
            dst = os.path.join(os.path.join(validation_dir,clss), fname)
            shutil.copyfile(src, dst)

        test_fnames = fnames[train_size : test_size + train_size]
        print(f'test size: {clss} file size: {len(test_fnames)}')

        for fname in test_fnames:
            src = os.path.join(path, fname)
            dst = os.path.join(os.path.join(test_dir, clss), fname)
            shutil.copyfile(src, dst)
