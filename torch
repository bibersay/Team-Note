# model save and load
#################################################
torch.save(model, path)                             # 모델 전체
torch.save(model.state_dict(), path)                # weight 만
torch.save(model.state_dict(), 'weights_only.pth')  # 사용방법

#model load
#################################################
model_new = torch.load(path.pth)                    # 모델 전체
model_new = NeuralNet()           # weight만 load 할때는 architecture가 필요함
model_new.load_state_dict(torch.load(path.pth)) 

####### cpu로 load
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
A.load_state_dict(torch.load('A.pth', map_location=device))   

#################################################
# PIL image, tensor, numpy 변환
from PIL import Image
from torchvision.transforms import ToTensor, ToPILImage
import numpy
import matplotlib.pyplot as plt


image = Image.open(file.jpg)    # PIL image open        # (0~255), (H,W,C)
np_image = numpy.array(image)   # numpy array           # (0~255), (H,W,C)
plt.imshow(image)               # PIL image show
plt.show()

tf_tensor = torchvision.transfroms.ToTensor()   # generate object       # (0~1), (C,H,W)
tf_img = tf_tensor(image)                       # convert from pil to tensor
tf_np_img = tf_tensor(np_image)                 # convert from pil to tensor
print(type(tf_img), tf_img,type(tf_np_img), tf_np_img)

tf_pil = torchvision.transfroms.ToPILImage()    # generate objec
tf_img = tf_pil(image)                          # convert from tensor to pil
tf_np_img = tf_pil(tf_np_img)                   # convert from tensor to pil
print(type(tf_img), tf_img, type(tf_np_img), tf_np_img)


#################################################

def preprocessing_datacopy(base_path, file_name):
"""
data copy and split
"""

    os.mkdir('/content/dataset')
    original_dataset_dir = os.path.join(base_path, file_name)

    classes_list = os.listdir(original_dataset_dir)

    base_dir = '/content/splitted'
    os.mkdir(base_dir)

    train_dir = os.path.join(base_dir, 'train')
    os.mkdir(train_dir)
    validation_dir = os.path.join(base_dir,'val')
    os.mkdir(validation_dir)
    test_dir = os.path.join(base_dir,'test')
    os.mkdir(test_dir)



    for clss in classes_list:
        os.mkdir(os.path.join(train_dir, clss))
        os.mkdir(os.path.join(validation_dir, clss))
        os.mkdir(os.path.join(test_dir, clss))


    for clss in classes_list:
        path = os.path.join(original_dataset_dir, clss)
        fnames = os.listdir(path)

        train_size = math.floor(len(fnames) * .6)
        validation_size = math.floor(len(fnames) * .2)
        test_size = math.floor(len(fnames) * .2)

        train_fnames = fnames[:train_size]
        print(f'train size: {clss}   file size: {len(train_fnames)}')
        for fname in train_fnames:
            src = os.path.join(path, fname)
            dst = os.path.join(os.path.join(train_dir, clss), fname)
            shutil.copyfile(src, dst)


        validation_fnames = fnames[train_size : validation_size + train_size]
        print(f'validation size: {clss} file size: {len(validation_fnames)}')

        for fname in validation_fnames:
            src = os.path.join(path, fname)
            dst = os.path.join(os.path.join(validation_dir,clss), fname)
            shutil.copyfile(src, dst)

        test_fnames = fnames[train_size : test_size + train_size]
        print(f'test size: {clss} file size: {len(test_fnames)}')

        for fname in test_fnames:
            src = os.path.join(path, fname)
            dst = os.path.join(os.path.join(test_dir, clss), fname)
            shutil.copyfile(src, dst)
